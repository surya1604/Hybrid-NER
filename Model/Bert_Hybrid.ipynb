{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surya1604/Hybrid-NER/blob/main/Model/Bert_Hybrid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iek8Bs1UOSNd",
        "outputId": "6e23fe79-51ea-42a0-bd15-028216f1dbd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: 2.0: No such file or directory\n",
            "Name: SQLAlchemy\n",
            "Version: 2.0.36\n",
            "Summary: Database Abstraction Library\n",
            "Home-page: https://www.sqlalchemy.org\n",
            "Author: Mike Bayer\n",
            "Author-email: mike_mp@zzzcomputing.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: greenlet, typing-extensions\n",
            "Required-by: alembic, bigframes, dataset, ipython-sql, langchain\n"
          ]
        }
      ],
      "source": [
        "!pip install sqlalchemy<2.0\n",
        "!pip show sqlalchemy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qykdvX-JAXPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpiRxGh0IkdR"
      },
      "outputs": [],
      "source": [
        "!pip install fsspec==2024.10.0\n",
        "!pip install transformers tokenizers seqeval -q\n",
        "!pip install datasets\n",
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB-xKFDmOBkb"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import re\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer, TrainerCallback\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0SPb1vRO7LY"
      },
      "outputs": [],
      "source": [
        "conll2003 = datasets.load_dataset(\"conll2003\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "metric = evaluate.load(\"seqeval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHRDRPciP1T9"
      },
      "outputs": [],
      "source": [
        "conll2003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yem-w4aWQF6w"
      },
      "outputs": [],
      "source": [
        "conll2003.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jl4q1IHQH02"
      },
      "outputs": [],
      "source": [
        "conll2003[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQVDQumjQJQZ"
      },
      "outputs": [],
      "source": [
        "conll2003[\"train\"].features[\"ner_tags\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7h5O4VYQKqP"
      },
      "outputs": [],
      "source": [
        "conll2003['train'].description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCpKL2coQMIr"
      },
      "outputs": [],
      "source": [
        "def rule_based_labeling(tokens):\n",
        "    labels = []\n",
        "    for token in tokens:\n",
        "        if re.match(r\"^[A-Z]+$\", token):  # All uppercase tokens\n",
        "            labels.append(3)  # Example label for `B-MISC`\n",
        "        elif re.match(r\".*\\d+.*\", token):  # Tokens containing numbers\n",
        "            labels.append(4)  # Example label for `B-NUM`\n",
        "        else:\n",
        "            labels.append(0)  # Default label (no entity)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pINbSmxjYUTw"
      },
      "outputs": [],
      "source": [
        "def apply_rules_and_merge(example):\n",
        "    tokens = example[\"tokens\"]\n",
        "    ner_tags = example[\"ner_tags\"]\n",
        "    rule_based_tags = rule_based_labeling(tokens)\n",
        "    merged_tags = [\n",
        "        rule_based_tag if rule_based_tag != 0 else ner_tag\n",
        "        for rule_based_tag, ner_tag in zip(rule_based_tags, ner_tags)\n",
        "    ]\n",
        "    example[\"ner_tags\"] = merged_tags\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR2nd5czYWOV"
      },
      "outputs": [],
      "source": [
        "conll2003 = conll2003.map(apply_rules_and_merge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7On9k15yQPQP"
      },
      "outputs": [],
      "source": [
        "conll2003['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8r5qddAsy-T"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)  # Special tokens\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])  # First token of a word\n",
        "            else:\n",
        "                label_ids.append(label[word_idx])  # Other tokens of a word\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWVY-RYQs1sO"
      },
      "outputs": [],
      "source": [
        "# Apply tokenization and alignment\n",
        "tokenized_datasets = conll2003.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "# Load Pretrained Model\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=len(conll2003[\"train\"].features[\"ner_tags\"].feature.names), hidden_dropout_prob=0.2\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiIiAwohs3eE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"test-ner\",\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
        "    learning_rate=3e-5,  # Fine-tuning learning rate\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,  # Training for fewer epochs to reduce overfitting\n",
        "    weight_decay=0.01,  # Regularization\n",
        "    save_strategy=\"epoch\",  # Save model after every epoch\n",
        "    load_best_model_at_end=True,  # Required for EarlyStoppingCallback\n",
        "    metric_for_best_model=\"eval_loss\",  # Monitor validation loss\n",
        "    logging_dir=\"./logs\",  # Directory for logging\n",
        "    save_total_limit=2,  # Keep only the last two saved models\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf1Z7uVRs5Rp"
      },
      "outputs": [],
      "source": [
        "# Define Data Collator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "# Compute Metrics Function\n",
        "label_list = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcDEzWAhs7l0"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    pred_logits, labels = eval_preds\n",
        "    pred_logits = np.argmax(pred_logits, axis=2)\n",
        "    predictions = [\n",
        "        [label_list[pred] for (pred, label) in zip(prediction, true_label) if label != -100]\n",
        "        for prediction, true_label in zip(pred_logits, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[label] for (pred, label) in zip(prediction, true_label) if label != -100]\n",
        "        for prediction, true_label in zip(pred_logits, labels)\n",
        "    ]\n",
        "    results = metric.compute(predictions=predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwWQ2-eotBlQ"
      },
      "outputs": [],
      "source": [
        "# Define a custom callback to log losses\n",
        "class LossLoggerCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.eval_losses = []\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None:\n",
        "            if \"loss\" in logs:  # Log training loss\n",
        "                self.train_losses.append(logs[\"loss\"])\n",
        "            if \"eval_loss\" in logs:  # Log evaluation loss\n",
        "                self.eval_losses.append(logs[\"eval_loss\"])\n",
        "\n",
        "# Initialize the callback\n",
        "loss_logger = LossLoggerCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "euAeeGW5tEHG",
        "outputId": "cb2a2e4f-7326-481d-cbbb-2aeddcf104a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-56-d3e075e6b7b4>:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5268' max='8780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5268/8780 15:36 < 10:24, 5.62 it/s, Epoch 6/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.261900</td>\n",
              "      <td>0.083841</td>\n",
              "      <td>0.918002</td>\n",
              "      <td>0.927841</td>\n",
              "      <td>0.922895</td>\n",
              "      <td>0.978172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.069000</td>\n",
              "      <td>0.073110</td>\n",
              "      <td>0.939538</td>\n",
              "      <td>0.938231</td>\n",
              "      <td>0.938884</td>\n",
              "      <td>0.981937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.040300</td>\n",
              "      <td>0.070357</td>\n",
              "      <td>0.941382</td>\n",
              "      <td>0.947312</td>\n",
              "      <td>0.944338</td>\n",
              "      <td>0.983605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.025200</td>\n",
              "      <td>0.072256</td>\n",
              "      <td>0.940738</td>\n",
              "      <td>0.949358</td>\n",
              "      <td>0.945028</td>\n",
              "      <td>0.983605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.018800</td>\n",
              "      <td>0.074625</td>\n",
              "      <td>0.944859</td>\n",
              "      <td>0.953285</td>\n",
              "      <td>0.949053</td>\n",
              "      <td>0.984876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.012600</td>\n",
              "      <td>0.076707</td>\n",
              "      <td>0.947142</td>\n",
              "      <td>0.952876</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.985051</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5268, training_loss=0.0605846011195114, metrics={'train_runtime': 936.9954, 'train_samples_per_second': 149.851, 'train_steps_per_second': 9.37, 'total_flos': 2045112348578508.0, 'train_loss': 0.0605846011195114, 'epoch': 6.0})"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        loss_logger,  # Custom callback\n",
        "        EarlyStoppingCallback(early_stopping_patience=3)  # Early stopping callback\n",
        "    ],  # Combine callbacks into a single list\n",
        ")\n",
        "\n",
        "# Train the Model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTkRIB61tErm"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(loss_logger.train_losses, label=\"Training Loss\", color=\"blue\", marker=\"o\")\n",
        "plt.plot(loss_logger.eval_losses, label=\"Validation Loss\", color=\"orange\", marker=\"o\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6T9oL_2TC0T"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}